{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is divided into two parts:**\n",
    "1. Model fitting dry run\n",
    "2. Testing model on random data\n",
    "3. Loading Real Data\n",
    "4. Making the model on the real data\n",
    "5. Testing the model on the real data\n",
    "6. Compresssing the model to be used in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GMM:\n",
    "    def __init__(self, n_components, max_iter=100, tol=1e-4):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self, X):\n",
    "        self._initialize_parameters(X)\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            prev_log_likelihood = self._compute_log_likelihood(X)\n",
    "            \n",
    "            # E-step: compute responsibilities\n",
    "            self.responsibilities = self._e_step(X)\n",
    "            \n",
    "            # M-step: update parameters\n",
    "            self._m_step(X)\n",
    "\n",
    "            # Check for convergence\n",
    "            log_likelihood = self._compute_log_likelihood(X)\n",
    "            if np.abs(log_likelihood - prev_log_likelihood) < self.tol:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_components))\n",
    "        for k in range(self.n_components):\n",
    "            probabilities[:, k] = self.weights[k] * self._gaussian(X, self.means[k], self.covariances[k])\n",
    "        cluster_labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "        # Create dictionary of clusters\n",
    "        clusters = {label: X[cluster_labels == label] for label in range(self.n_components)}\n",
    "        return clusters\n",
    "\n",
    "    def _initialize_parameters(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.means = X[np.random.choice(n_samples, self.n_components, replace=False)]\n",
    "        self.covariances = np.array([np.eye(n_features) for _ in range(self.n_components)])\n",
    "        self.weights = np.full(self.n_components, 1 / self.n_components)\n",
    "\n",
    "    def _e_step(self, X):\n",
    "        responsibilities = np.zeros((X.shape[0], self.n_components))\n",
    "        for k in range(self.n_components):\n",
    "            responsibilities[:, k] = self.weights[k] * self._gaussian(X, self.means[k], self.covariances[k])\n",
    "        responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
    "        return responsibilities\n",
    "\n",
    "    def _m_step(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        for k in range(self.n_components):\n",
    "            responsibility = self.responsibilities[:, k]\n",
    "            total_responsibility = responsibility.sum()\n",
    "            self.means[k] = (responsibility[:, np.newaxis] * X).sum(axis=0) / total_responsibility\n",
    "            self.covariances[k] = (\n",
    "                (responsibility[:, np.newaxis, np.newaxis] * (X - self.means[k])[:, :, np.newaxis] * (X - self.means[k])[:, np.newaxis, :]).sum(axis=0)\n",
    "                / total_responsibility\n",
    "            )\n",
    "            self.weights[k] = total_responsibility / n_samples\n",
    "\n",
    "    def _compute_log_likelihood(self, X):\n",
    "        likelihood = np.zeros(X.shape[0])\n",
    "        for k in range(self.n_components):\n",
    "            likelihood += self.weights[k] * self._gaussian(X, self.means[k], self.covariances[k])\n",
    "        return np.sum(np.log(likelihood))\n",
    "\n",
    "    @staticmethod\n",
    "    def _gaussian(X, mean, covariance):\n",
    "        n_features = X.shape[1]\n",
    "        determinant = np.linalg.det(covariance)\n",
    "        inverse = np.linalg.inv(covariance)\n",
    "        norm_factor = (2 * np.pi) ** (-n_features / 2) * determinant ** -0.5\n",
    "        diff = X - mean\n",
    "        return norm_factor * np.exp(-0.5 * np.sum(diff @ inverse * diff, axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([[ 5. ,  8. ],\n",
      "       [ 6. ,  8.5],\n",
      "       [ 9. , 11. ],\n",
      "       [ 8.5, 10.5],\n",
      "       [ 7.5,  7. ],\n",
      "       [ 3.5,  5.5]]), 1: array([[1. , 2. ],\n",
      "       [1.5, 1.8],\n",
      "       [2. , 2.5],\n",
      "       [4.5, 3. ]])}\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [1.0, 2.0],\n",
    "    [1.5, 1.8],\n",
    "    [5.0, 8.0],\n",
    "    [6.0, 8.5],\n",
    "    [9.0, 11.0],\n",
    "    [8.5, 10.5],\n",
    "    [2.0, 2.5],  # Adding points with more variance in both dimensions\n",
    "    [7.5, 7.0],  # to avoid covariance matrix becoming singular\n",
    "    [3.5, 5.5],\n",
    "    [4.5, 3.0]\n",
    "])\n",
    "\n",
    "model = GMM(n_components=2)\n",
    "model.fit(X)\n",
    "clusters = model.predict(X)\n",
    "\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Begins the text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Choose a suitable model\n",
    "text_paragraphs = [\n",
    "    \"Why do we need softmax?\",\n",
    "    \"Explain the Gaussian Mixture Model algorithm.\",\n",
    "    \"What are supervised learning techniques?\",\n",
    "    \"How does rule-based AI work?\",\n",
    "    \"what are netwrok sockets?\",\n",
    "    \"what is a web server?\",\n",
    "    \"How do we know if an index for a database is useful?\",\n",
    "    \"What is the difference between a primary key and a foreign key?\",\n",
    "    \"What is a surrogate key?\",\n",
    "    \"Who discovered vector spaces?\",\n",
    "]\n",
    "embeddings = model.encode(text_paragraphs)  # Shape: (n_samples, embedding_dim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 384)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Autoencoder:\n",
    "    def __init__(self, input_dim=384, hidden_dim=100, learning_rate=0.01, epochs=1000):\n",
    "        self.input_dim = input_dim      # 384 features\n",
    "        self.hidden_dim = hidden_dim    # 100 features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # Initialize weights and biases for encoder and decoder\n",
    "        # Use He initialization for better training\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2./input_dim)   # (384, 100)\n",
    "        self.b1 = np.zeros((1, hidden_dim))                                         # (1, 100)\n",
    "        self.W2 = np.random.randn(hidden_dim, input_dim) * np.sqrt(2./hidden_dim)  # (100, 384)\n",
    "        self.b2 = np.zeros((1, input_dim))                                         # (1, 384)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # Clip values to avoid overflow\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def train(self, X):\n",
    "        if X.shape[1] != self.input_dim:\n",
    "            raise ValueError(f\"Input dimension mismatch. Expected {self.input_dim}, got {X.shape[1]}\")\n",
    "            \n",
    "        losses = []\n",
    "        m = X.shape[0]  # number of training examples\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            # Forward pass\n",
    "            # Encoder: (m, 384) @ (384, 100) -> (m, 100)\n",
    "            encoded = self.sigmoid(np.dot(X, self.W1) + self.b1)\n",
    "            \n",
    "            # Decoder: (m, 100) @ (100, 384) -> (m, 384)\n",
    "            decoded = self.sigmoid(np.dot(encoded, self.W2) + self.b2)\n",
    "            \n",
    "            # Compute reconstruction loss (mean squared error)\n",
    "            loss = np.mean((X - decoded) ** 2)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backpropagation\n",
    "            # Output layer error\n",
    "            error = X - decoded\n",
    "            d_decoded = error * self.sigmoid_derivative(decoded)\n",
    "            \n",
    "            # Decoder gradients\n",
    "            # (100, m) @ (m, 384) -> (100, 384)\n",
    "            dW2 = np.dot(encoded.T, d_decoded) / m\n",
    "            db2 = np.sum(d_decoded, axis=0, keepdims=True) / m\n",
    "            \n",
    "            # Hidden layer error\n",
    "            # (m, 384) @ (384, 100) -> (m, 100)\n",
    "            d_encoded = np.dot(d_decoded, self.W2.T) * self.sigmoid_derivative(encoded)\n",
    "            \n",
    "            # Encoder gradients\n",
    "            # (384, m) @ (m, 100) -> (384, 100)\n",
    "            dW1 = np.dot(X.T, d_encoded) / m\n",
    "            db1 = np.sum(d_encoded, axis=0, keepdims=True) / m\n",
    "            \n",
    "            # Update weights and biases with gradient descent\n",
    "            self.W1 += self.learning_rate * dW1\n",
    "            self.b1 += self.learning_rate * db1\n",
    "            self.W2 += self.learning_rate * dW2\n",
    "            self.b2 += self.learning_rate * db2\n",
    "            \n",
    "            # Print progress\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.6f}\")\n",
    "                \n",
    "        return losses\n",
    "\n",
    "    def encode(self, X):\n",
    "        \"\"\"Convert input data from 384 dimensions to 100 dimensions\"\"\"\n",
    "        if X.shape[1] != self.input_dim:\n",
    "            raise ValueError(f\"Input dimension mismatch. Expected {self.input_dim}, got {X.shape[1]}\")\n",
    "        return self.sigmoid(np.dot(X, self.W1) + self.b1)\n",
    "    \n",
    "    def decode(self, encoded):\n",
    "        \"\"\"Convert encoded 100-dimensional data back to 384 dimensions\"\"\"\n",
    "        if encoded.shape[1] != self.hidden_dim:\n",
    "            raise ValueError(f\"Encoded dimension mismatch. Expected {self.hidden_dim}, got {encoded.shape[1]}\")\n",
    "        return self.sigmoid(np.dot(encoded, self.W2) + self.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 0.892509\n",
      "Epoch 200/1000, Loss: 0.822409\n",
      "Epoch 300/1000, Loss: 0.771816\n",
      "Epoch 400/1000, Loss: 0.736100\n",
      "Epoch 500/1000, Loss: 0.710294\n",
      "Epoch 600/1000, Loss: 0.690995\n",
      "Epoch 700/1000, Loss: 0.676085\n",
      "Epoch 800/1000, Loss: 0.664524\n",
      "Epoch 900/1000, Loss: 0.655470\n",
      "Epoch 1000/1000, Loss: 0.648216\n",
      "Original shape: (10, 384)\n",
      "Encoded shape: (10, 100)\n",
      "Decoded shape: (10, 384)\n"
     ]
    }
   ],
   "source": [
    "# Create example data (10 samples, 384 features)\n",
    "X = np.random.randn(10, 384)\n",
    "\n",
    "# Initialize and train autoencoder\n",
    "autoencoder = Autoencoder(input_dim=384, hidden_dim=100, learning_rate=0.01, epochs=1000)\n",
    "losses = autoencoder.train(X)\n",
    "\n",
    "# Encode data to reduced dimension\n",
    "encoded_data = autoencoder.encode(X)  # Shape: (10, 100)\n",
    "\n",
    "# Decode back to original dimension\n",
    "decoded_data = autoencoder.decode(encoded_data)  # Shape: (10, 384)\n",
    "\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"Encoded shape:\", encoded_data.shape)\n",
    "print(\"Decoded shape:\", decoded_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Normalizes the dataset by scaling each feature to a range of 0 to 1 \n",
    "    using min-max normalization.\n",
    "    \n",
    "    Parameters:\n",
    "    dataset (numpy.ndarray): A 2D array of shape (n_samples, n_features) where n_features = 100.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A normalized dataset with feature values scaled to the range [0, 1].\n",
    "    \"\"\"\n",
    "    if dataset.shape[1] != 100:\n",
    "        raise ValueError(\"The dataset must have exactly 100 features.\")\n",
    "    \n",
    "    # Calculate the min and max for each feature (axis=0)\n",
    "    feature_mins = np.min(dataset, axis=0)\n",
    "    feature_maxs = np.max(dataset, axis=0)\n",
    "    \n",
    "    # Avoid division by zero for features with constant values\n",
    "    feature_ranges = feature_maxs - feature_mins\n",
    "    feature_ranges[feature_ranges == 0] = 1  # Prevent division by zero\n",
    "    \n",
    "    # Normalize the dataset\n",
    "    normalized_data = (dataset - feature_mins) / feature_ranges\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_data = normalize_dataset(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value1\n",
      "value2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NumPyArrayHashTable:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.table = [None] * size\n",
    "\n",
    "    def _hash(self, key):\n",
    "        # Implement a suitable hash function for NumPy arrays\n",
    "        # This example uses a simple hash based on the array's data\n",
    "        return hash(key.tobytes()) % self.size\n",
    "\n",
    "    def insert(self, key, value):\n",
    "        index = self._hash(key)\n",
    "        if self.table[index] is None:\n",
    "            self.table[index] = [(key, value)]\n",
    "        else:\n",
    "            self.table[index].append((key, value))\n",
    "\n",
    "    def get(self, key):\n",
    "        index = self._hash(key)\n",
    "        if self.table[index]:\n",
    "            for k, v in self.table[index]:\n",
    "                if np.array_equal(k, key):\n",
    "                    return v\n",
    "        return None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"NumPyArrayHashTable(size={self.size}, table={self.table})\"\n",
    "\n",
    "# Example usage\n",
    "ht = NumPyArrayHashTable(10)\n",
    "key1 = np.array([1, 2, 3])\n",
    "key2 = np.array([4, 5, 6])\n",
    "ht.insert(key1, \"value1\")\n",
    "ht.insert(key2, \"value2\")\n",
    "\n",
    "print(ht.get(key1))  # Output: \"value1\"\n",
    "print(ht.get(key2))  # Output: \"value2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping embeddings to their original sentences\n",
    "# Initialize hash table with size equal to number of embeddings\n",
    "hash_table = NumPyArrayHashTable(len(stan_data))\n",
    "\n",
    "# Insert each embedding-text pair into the hash table\n",
    "for embedding, text in zip(stan_data, text_paragraphs):\n",
    "    hash_table.insert(embedding, text)\n",
    "\n",
    "embedding_to_sentence = hash_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rijusmit\\AppData\\Local\\Temp\\ipykernel_35236\\130840163.py:72: RuntimeWarning: divide by zero encountered in scalar power\n",
      "  norm_factor = (2 * np.pi) ** (-n_features / 2) * determinant ** -0.5\n",
      "C:\\Users\\rijusmit\\AppData\\Local\\Temp\\ipykernel_35236\\130840163.py:74: RuntimeWarning: invalid value encountered in multiply\n",
      "  return norm_factor * np.exp(-0.5 * np.sum(diff @ inverse * diff, axis=1))\n",
      "C:\\Users\\rijusmit\\AppData\\Local\\Temp\\ipykernel_35236\\130840163.py:74: RuntimeWarning: overflow encountered in exp\n",
      "  return norm_factor * np.exp(-0.5 * np.sum(diff @ inverse * diff, axis=1))\n",
      "C:\\Users\\rijusmit\\AppData\\Local\\Temp\\ipykernel_35236\\130840163.py:46: RuntimeWarning: invalid value encountered in divide\n",
      "  responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "text_model = GMM(n_components=6)\n",
    "text_model.fit(stan_data)\n",
    "text_clusters = text_model.predict(stan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[1.39823518e-02, 1.32726269e-02, 9.54514062e-01, 2.36157203e-04,\n",
       "         0.00000000e+00, 9.92457501e-01, 9.96435118e-01, 9.91835862e-01,\n",
       "         1.28901974e-04, 1.00000000e+00, 5.18177690e-04, 2.78358437e-04,\n",
       "         5.13162036e-03, 9.96507949e-01, 1.00000000e+00, 0.00000000e+00,\n",
       "         9.98824165e-01, 1.18611350e-02, 9.92050379e-01, 9.99919676e-01,\n",
       "         9.96200551e-01, 0.00000000e+00, 9.90265037e-01, 9.90532489e-01,\n",
       "         9.99279014e-01, 9.80329242e-01, 1.00000000e+00, 1.09615794e-02,\n",
       "         1.00000000e+00, 9.96478266e-01, 9.91759718e-01, 2.28734797e-03,\n",
       "         9.94092925e-01, 2.42789146e-03, 9.90798784e-01, 0.00000000e+00,\n",
       "         2.05655091e-03, 0.00000000e+00, 9.95638964e-01, 2.16577958e-03,\n",
       "         3.60858053e-03, 2.55812553e-03, 9.78764406e-01, 1.00000000e+00,\n",
       "         9.88085640e-01, 9.99286629e-01, 9.97537032e-01, 1.00000000e+00,\n",
       "         9.96242973e-01, 2.44637670e-03, 8.10998650e-04, 1.14474362e-02,\n",
       "         5.93951850e-04, 9.96089282e-01, 0.00000000e+00, 9.95282384e-01,\n",
       "         1.12757029e-02, 9.99186863e-01, 0.00000000e+00, 1.00000000e+00,\n",
       "         9.93835562e-01, 9.94046286e-01, 9.98580130e-01, 9.98471465e-01,\n",
       "         9.93134439e-01, 1.00000000e+00, 9.96734089e-01, 9.95505987e-01,\n",
       "         0.00000000e+00, 0.00000000e+00, 3.97254090e-02, 9.95095710e-01,\n",
       "         1.00000000e+00, 5.47996362e-03, 1.91714675e-03, 1.00000000e+00,\n",
       "         1.43629084e-04, 0.00000000e+00, 1.00000000e+00, 6.60651158e-03,\n",
       "         2.58015153e-03, 5.56159400e-03, 1.00000000e+00, 0.00000000e+00,\n",
       "         7.53746013e-03, 9.89342182e-01, 9.91750923e-01, 3.41956261e-03,\n",
       "         0.00000000e+00, 9.99540441e-01, 9.95849060e-01, 9.97302067e-01,\n",
       "         9.98348525e-01, 0.00000000e+00, 9.98020195e-01, 9.92804413e-01,\n",
       "         4.95361819e-04, 1.00000000e+00, 9.98317343e-01, 9.24258457e-01],\n",
       "        [9.81040983e-01, 1.00000000e+00, 8.23137484e-01, 9.98995712e-01,\n",
       "         5.22680464e-01, 1.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         1.00000000e+00, 9.99927764e-01, 0.00000000e+00, 9.98002227e-01,\n",
       "         1.00000000e+00, 3.09333660e-03, 9.90492442e-01, 9.97515614e-01,\n",
       "         9.99900091e-01, 7.51905823e-05, 9.45844901e-03, 4.73630196e-04,\n",
       "         1.14515855e-03, 1.46428921e-03, 3.03212449e-03, 9.95359560e-01,\n",
       "         1.00000000e+00, 9.98478458e-01, 9.95025284e-01, 9.84585450e-01,\n",
       "         9.96375783e-01, 3.14667048e-03, 9.92528278e-01, 1.29124206e-01,\n",
       "         3.95713446e-03, 9.95671292e-01, 1.00000000e+00, 9.99429919e-01,\n",
       "         1.00000000e+00, 1.86299255e-03, 9.95674519e-01, 9.90778457e-01,\n",
       "         0.00000000e+00, 0.00000000e+00, 9.96699924e-01, 0.00000000e+00,\n",
       "         1.00000000e+00, 3.91772583e-03, 4.91035323e-03, 2.66253550e-03,\n",
       "         4.20904198e-03, 9.98073812e-01, 9.95315354e-01, 9.97580532e-01,\n",
       "         9.89163169e-01, 9.95232067e-01, 9.99922939e-01, 6.73332534e-03,\n",
       "         9.99831502e-01, 1.00000000e+00, 9.99562371e-01, 9.97006302e-01,\n",
       "         9.96516521e-01, 1.00000000e+00, 9.99697464e-01, 1.30491148e-03,\n",
       "         9.86403130e-01, 0.00000000e+00, 1.00000000e+00, 9.85255646e-01,\n",
       "         1.00000000e+00, 2.40168639e-03, 9.98443784e-01, 9.97523919e-01,\n",
       "         9.96029531e-01, 2.05131928e-03, 9.94615902e-01, 9.95698247e-01,\n",
       "         9.97876546e-01, 1.61437528e-02, 9.92845736e-01, 9.98083902e-01,\n",
       "         2.72946468e-03, 8.02120372e-04, 0.00000000e+00, 4.79599238e-03,\n",
       "         0.00000000e+00, 4.06029824e-03, 9.99659778e-01, 9.97793637e-01,\n",
       "         1.65209790e-03, 9.98876319e-01, 9.99966969e-01, 0.00000000e+00,\n",
       "         1.60826586e-03, 9.96737590e-01, 1.00000000e+00, 8.20266796e-04,\n",
       "         9.97078473e-01, 9.92127966e-01, 2.06477774e-03, 9.93323436e-01],\n",
       "        [4.98197842e-03, 9.99534120e-01, 9.97547134e-01, 9.97835986e-01,\n",
       "         1.53364981e-03, 9.93720025e-01, 9.96324749e-01, 9.49186808e-01,\n",
       "         9.99721435e-01, 5.29879608e-03, 1.00000000e+00, 1.00000000e+00,\n",
       "         9.95722765e-01, 9.94613761e-01, 9.98090369e-01, 7.98837686e-04,\n",
       "         0.00000000e+00, 9.99537422e-01, 2.98595750e-04, 9.93333544e-04,\n",
       "         0.00000000e+00, 4.50944556e-01, 9.98431247e-01, 1.00000000e+00,\n",
       "         0.00000000e+00, 9.97910730e-01, 9.95732140e-01, 3.43338012e-02,\n",
       "         1.89281614e-03, 3.26150832e-03, 7.44338600e-03, 1.54899638e-01,\n",
       "         9.97282063e-01, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "         9.98255167e-01, 3.09842038e-03, 1.00000000e+00, 9.99295860e-01,\n",
       "         2.59822754e-03, 1.00000000e+00, 3.48889920e-03, 9.99985597e-01,\n",
       "         0.00000000e+00, 1.09100835e-02, 9.98883207e-01, 1.74870598e-03,\n",
       "         9.95732213e-01, 1.45085406e-03, 1.11916150e-03, 5.66741843e-04,\n",
       "         9.92045878e-01, 1.00000000e+00, 9.94093770e-01, 9.99903023e-01,\n",
       "         2.80873639e-04, 0.00000000e+00, 9.95217374e-01, 2.99657571e-03,\n",
       "         0.00000000e+00, 9.96135624e-01, 3.81081992e-03, 9.91709815e-01,\n",
       "         9.97763071e-01, 8.08370538e-03, 9.96059570e-01, 1.19637308e-03,\n",
       "         9.95211093e-01, 1.79213765e-03, 9.97655412e-01, 1.32994726e-03,\n",
       "         5.80921670e-02, 3.89847251e-03, 9.95522956e-01, 9.98588112e-01,\n",
       "         1.02576006e-03, 9.97988413e-01, 9.99277286e-01, 1.00000000e+00,\n",
       "         9.99157472e-01, 5.36531883e-03, 1.36166085e-03, 9.99684677e-01,\n",
       "         2.22262616e-03, 9.99654667e-01, 1.66651954e-03, 1.50345253e-03,\n",
       "         9.98176446e-01, 2.40061280e-02, 0.00000000e+00, 5.88586301e-03,\n",
       "         0.00000000e+00, 9.97209332e-01, 1.92641040e-03, 4.67381544e-03,\n",
       "         9.97957714e-01, 9.96783862e-01, 1.00000000e+00, 9.96289334e-01],\n",
       "        [9.93707207e-01, 3.08031515e-03, 9.89306032e-01, 0.00000000e+00,\n",
       "         4.21987887e-04, 2.80602764e-04, 9.92529066e-01, 1.00000000e+00,\n",
       "         9.97518679e-01, 9.78412159e-01, 9.99441624e-01, 9.92598009e-01,\n",
       "         0.00000000e+00, 1.27733427e-03, 9.93917476e-01, 9.58785059e-01,\n",
       "         1.08464746e-03, 2.52465349e-03, 1.24506312e-03, 1.00000000e+00,\n",
       "         1.00000000e+00, 3.10762703e-03, 9.98834874e-01, 9.97062925e-01,\n",
       "         9.87898816e-01, 3.11730303e-04, 9.47201750e-04, 1.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 8.10205641e-01, 9.95775214e-01,\n",
       "         9.91998952e-01, 9.95851646e-01, 3.99691499e-03, 9.96332665e-01,\n",
       "         2.67944789e-03, 9.96488223e-01, 9.97727921e-01, 1.81176627e-03,\n",
       "         1.00000000e+00, 9.97986885e-01, 9.96483674e-01, 9.99503147e-04,\n",
       "         9.96246922e-01, 2.25232310e-03, 9.76474028e-03, 2.02450752e-03,\n",
       "         2.52327344e-02, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         8.54384101e-03, 9.97012683e-01, 9.99407331e-01, 0.00000000e+00,\n",
       "         4.90348605e-04, 9.93547807e-03, 1.00000000e+00, 0.00000000e+00,\n",
       "         9.94924307e-01, 4.03421738e-03, 1.90402515e-03, 9.99412319e-01,\n",
       "         9.93820356e-01, 9.91478512e-01, 9.97127795e-01, 7.97327889e-03,\n",
       "         9.98112140e-01, 9.95817374e-01, 1.56562138e-02, 9.97884197e-01,\n",
       "         0.00000000e+00, 2.80565518e-03, 1.29557564e-03, 1.83136552e-03,\n",
       "         9.92254950e-01, 1.00000000e+00, 0.00000000e+00, 2.80018711e-03,\n",
       "         9.98511726e-01, 9.99557689e-01, 9.99143663e-01, 1.00000000e+00,\n",
       "         9.99948349e-01, 1.00000000e+00, 9.95027461e-01, 0.00000000e+00,\n",
       "         1.00000000e+00, 0.00000000e+00, 1.20417971e-03, 1.00000000e+00,\n",
       "         9.94917989e-01, 9.89251079e-01, 9.92995690e-01, 9.97309716e-01,\n",
       "         9.99376426e-01, 1.21938778e-02, 9.94261362e-01, 4.41900375e-03],\n",
       "        [2.07933701e-03, 9.98987754e-01, 4.12650127e-03, 4.20796209e-03,\n",
       "         9.82283619e-01, 9.97248044e-01, 9.91673710e-01, 9.94940779e-01,\n",
       "         6.44708542e-03, 3.06810384e-03, 9.90631336e-01, 9.98145754e-01,\n",
       "         7.98403022e-01, 9.68346147e-01, 9.86479023e-01, 1.46011908e-02,\n",
       "         9.90580771e-01, 9.98301519e-01, 9.78341569e-02, 9.92467922e-01,\n",
       "         9.97745235e-01, 9.94610194e-01, 2.74801967e-03, 9.96811839e-01,\n",
       "         9.99428209e-01, 9.98225227e-01, 3.08255364e-03, 9.93760435e-01,\n",
       "         9.94086652e-01, 9.93435945e-01, 5.82416525e-03, 9.96540869e-01,\n",
       "         9.94103755e-01, 9.96985888e-01, 9.96403267e-01, 4.17742049e-03,\n",
       "         0.00000000e+00, 9.99650762e-01, 9.99730214e-01, 9.94798349e-01,\n",
       "         9.98965229e-01, 1.60235828e-03, 9.89972782e-01, 1.04004200e-03,\n",
       "         9.91569944e-01, 3.21500126e-04, 9.98315960e-01, 0.00000000e+00,\n",
       "         9.86425280e-01, 9.97748799e-01, 9.94904398e-01, 9.96316381e-01,\n",
       "         0.00000000e+00, 2.70251524e-02, 9.96628204e-01, 1.88555541e-03,\n",
       "         9.97636584e-01, 9.94287192e-01, 9.90442200e-01, 9.94386467e-01,\n",
       "         2.33422080e-03, 9.95279000e-01, 1.00000000e+00, 5.63768087e-03,\n",
       "         0.00000000e+00, 7.38131388e-03, 0.00000000e+00, 9.96295689e-01,\n",
       "         9.93254064e-01, 1.00000000e+00, 1.37373205e-03, 3.08888345e-03,\n",
       "         9.99841352e-01, 8.30770046e-01, 9.89919343e-01, 4.46067035e-03,\n",
       "         4.41064533e-03, 9.97431096e-01, 5.93509529e-04, 1.29048809e-03,\n",
       "         9.92904224e-01, 9.98392949e-01, 9.98636172e-01, 9.04187842e-02,\n",
       "         9.89435488e-01, 1.28446630e-03, 2.04667379e-02, 5.62788520e-03,\n",
       "         9.95453442e-01, 1.00000000e+00, 9.91921602e-01, 7.72950822e-03,\n",
       "         9.76405084e-01, 4.48054094e-04, 5.82548519e-03, 4.43579387e-04,\n",
       "         2.09668671e-03, 0.00000000e+00, 8.61429221e-04, 9.78437216e-01],\n",
       "        [9.97766290e-01, 9.91487672e-01, 9.97846316e-01, 9.98034718e-01,\n",
       "         9.85021535e-01, 3.56614129e-04, 2.33173125e-03, 9.97487100e-01,\n",
       "         3.22030499e-03, 9.98401883e-01, 8.67384577e-01, 0.00000000e+00,\n",
       "         9.93762129e-01, 0.00000000e+00, 4.41794632e-04, 1.18684593e-03,\n",
       "         9.98751893e-01, 1.57464178e-02, 5.84896431e-03, 6.58015501e-03,\n",
       "         1.51752680e-02, 1.40717427e-03, 1.91773543e-02, 3.03344559e-03,\n",
       "         6.58029081e-03, 3.32268648e-03, 9.98908893e-01, 9.97282535e-01,\n",
       "         6.29005642e-04, 4.80736876e-04, 2.74938446e-03, 9.87213351e-01,\n",
       "         1.58045381e-03, 9.94372102e-01, 9.91773978e-01, 1.04722051e-02,\n",
       "         9.76986083e-01, 5.05293908e-03, 3.98188549e-03, 9.97767009e-01,\n",
       "         9.99099412e-01, 9.93800800e-01, 9.91795200e-01, 9.98897239e-01,\n",
       "         1.17786526e-03, 9.99351042e-01, 1.88452653e-03, 9.98426915e-01,\n",
       "         1.00000000e+00, 9.89257119e-01, 9.97864778e-01, 3.47104786e-04,\n",
       "         9.98043047e-01, 9.94884994e-01, 2.25896791e-03, 9.74761180e-01,\n",
       "         9.78414510e-01, 9.95353318e-01, 1.72879241e-03, 8.97802409e-02,\n",
       "         1.00000000e+00, 1.51588254e-03, 1.27223388e-03, 1.00032579e-01,\n",
       "         9.98865710e-01, 9.98331162e-01, 9.95451356e-01, 1.00000000e+00,\n",
       "         9.75703885e-01, 9.97918626e-01, 1.00000000e+00, 9.98151825e-01,\n",
       "         3.34442949e-03, 9.86737370e-01, 9.89386338e-01, 1.96714282e-03,\n",
       "         0.00000000e+00, 7.03043351e-03, 9.97895889e-01, 9.97429927e-01,\n",
       "         9.70766955e-01, 6.42700622e-04, 8.41644313e-01, 2.10175701e-02,\n",
       "         9.93460748e-01, 9.96941120e-01, 1.00000000e+00, 1.83168930e-04,\n",
       "         9.96201246e-01, 9.26989370e-01, 9.84375381e-01, 9.49677228e-03,\n",
       "         9.98370711e-01, 9.86692061e-01, 2.66736093e-03, 0.00000000e+00,\n",
       "         1.00000000e+00, 9.93802022e-01, 4.31514255e-03, 1.30891143e-03],\n",
       "        [0.00000000e+00, 3.24052750e-03, 9.63850640e-01, 1.00000000e+00,\n",
       "         1.00000000e+00, 9.97448883e-01, 0.00000000e+00, 9.75773258e-01,\n",
       "         0.00000000e+00, 9.91823993e-01, 9.93035377e-01, 1.28305843e-03,\n",
       "         8.93116292e-01, 9.97127964e-01, 0.00000000e+00, 1.00000000e+00,\n",
       "         9.54451463e-01, 0.00000000e+00, 1.00000000e+00, 7.76229373e-04,\n",
       "         9.94177770e-01, 2.24641345e-03, 9.90412535e-01, 0.00000000e+00,\n",
       "         3.42609854e-04, 9.98675213e-01, 0.00000000e+00, 1.42958518e-03,\n",
       "         2.81950448e-03, 9.98738812e-01, 1.00000000e+00, 8.36624968e-04,\n",
       "         0.00000000e+00, 2.42018634e-03, 9.95586725e-01, 9.99939384e-01,\n",
       "         9.98524069e-01, 1.00000000e+00, 0.00000000e+00, 3.89829436e-03,\n",
       "         8.69029989e-04, 3.00540695e-03, 9.91113771e-01, 6.39936775e-03,\n",
       "         9.97987279e-01, 1.00000000e+00, 0.00000000e+00, 9.90459893e-01,\n",
       "         9.98104995e-01, 1.00000000e+00, 4.75847065e-03, 1.00000000e+00,\n",
       "         1.00000000e+00, 0.00000000e+00, 9.99679068e-01, 9.96477835e-01,\n",
       "         1.46378013e-03, 1.25392194e-03, 9.98038706e-01, 1.92636564e-03,\n",
       "         9.98653264e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.00000000e+00, 5.51086607e-01, 9.99380698e-01, 0.00000000e+00,\n",
       "         9.98657206e-01, 9.97638007e-01, 1.08610507e-03, 1.00000000e+00,\n",
       "         4.10542125e-03, 1.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         1.00000000e+00, 9.97897535e-01, 3.25066395e-03, 9.99591811e-01,\n",
       "         9.73819411e-01, 2.21951629e-03, 2.37359958e-03, 9.97064963e-01,\n",
       "         9.97856568e-01, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "         9.98567351e-01, 5.64154029e-04, 4.88086461e-03, 9.94655587e-01,\n",
       "         9.99908206e-01, 1.00000000e+00, 9.96531201e-01, 9.99902256e-01,\n",
       "         9.97597591e-01, 1.79622834e-03, 0.00000000e+00, 9.97344212e-01],\n",
       "        [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.96292005e-01,\n",
       "         9.97593207e-01, 9.95288685e-01, 5.32986041e-03, 9.76583433e-01,\n",
       "         9.97932826e-01, 9.95645666e-01, 9.08417137e-04, 9.97112121e-01,\n",
       "         9.85474062e-01, 9.87367685e-03, 2.58207612e-03, 9.99133488e-01,\n",
       "         9.40560693e-04, 1.00000000e+00, 0.00000000e+00, 9.99725166e-01,\n",
       "         1.17429011e-02, 1.00000000e+00, 1.00000000e+00, 1.83358808e-03,\n",
       "         9.96639267e-01, 1.06226413e-03, 9.96522338e-01, 0.00000000e+00,\n",
       "         5.87506835e-03, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         9.96023411e-01, 9.85818376e-01, 9.94586990e-01, 1.67851192e-03,\n",
       "         9.86557799e-01, 9.94118580e-01, 6.83106773e-04, 1.00000000e+00,\n",
       "         9.86534924e-01, 9.83912047e-01, 0.00000000e+00, 9.98535248e-01,\n",
       "         1.25744355e-02, 8.23959104e-04, 9.99705607e-01, 4.64315998e-02,\n",
       "         0.00000000e+00, 9.97674283e-01, 9.96859580e-01, 9.97544833e-01,\n",
       "         9.99284078e-01, 1.87803222e-02, 1.00000000e+00, 1.00000000e+00,\n",
       "         1.00000000e+00, 1.37850465e-02, 9.87386721e-01, 1.65687349e-02,\n",
       "         5.24245484e-03, 9.99126861e-01, 9.99364251e-01, 3.61247635e-03,\n",
       "         2.58421616e-03, 9.95349636e-01, 9.88883353e-01, 2.11225933e-04,\n",
       "         3.71654495e-03, 9.94198208e-01, 9.97411563e-01, 0.00000000e+00,\n",
       "         5.15075598e-03, 0.00000000e+00, 9.94044855e-01, 9.93702789e-01,\n",
       "         9.94249962e-01, 9.76840300e-01, 2.51939673e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 5.39682245e-01, 9.95641358e-01,\n",
       "         8.64363628e-01, 9.53159663e-01, 9.96165237e-01, 6.36089606e-04,\n",
       "         9.97899541e-01, 1.21239882e-03, 1.00000000e+00, 4.81656399e-03,\n",
       "         1.00000000e+00, 2.93535534e-03, 8.13811419e-03, 1.00000000e+00,\n",
       "         0.00000000e+00, 2.04665102e-03, 9.92971752e-01, 0.00000000e+00],\n",
       "        [9.99219915e-01, 9.88910004e-01, 1.00000000e+00, 9.93362203e-01,\n",
       "         7.12690395e-03, 1.15637436e-02, 9.99311645e-01, 8.81934688e-01,\n",
       "         1.36620742e-03, 9.86144084e-01, 9.93221692e-01, 9.94157465e-01,\n",
       "         7.16155543e-02, 1.00000000e+00, 7.98986912e-02, 9.93473950e-01,\n",
       "         2.32469759e-03, 9.99479370e-01, 2.21116139e-03, 0.00000000e+00,\n",
       "         1.84270436e-03, 3.55316178e-03, 0.00000000e+00, 9.97294483e-01,\n",
       "         1.91166190e-02, 0.00000000e+00, 9.93676924e-04, 3.15296223e-03,\n",
       "         2.13459918e-01, 9.98762807e-01, 9.97722305e-01, 1.00000000e+00,\n",
       "         1.00000000e+00, 9.91143164e-01, 9.92668686e-01, 7.17535641e-03,\n",
       "         9.92035814e-01, 1.03477445e-03, 9.97815308e-01, 0.00000000e+00,\n",
       "         9.99791292e-01, 9.98473678e-01, 1.00000000e+00, 9.99772853e-01,\n",
       "         7.59018718e-02, 0.00000000e+00, 1.00000000e+00, 9.98254063e-01,\n",
       "         2.88439868e-03, 9.94501520e-01, 0.00000000e+00, 1.12322418e-03,\n",
       "         9.74753200e-01, 9.98467363e-01, 8.88061340e-04, 9.83219840e-01,\n",
       "         9.99013005e-01, 8.75186856e-03, 3.10610106e-03, 9.97787751e-01,\n",
       "         4.07902037e-03, 9.99693169e-01, 1.60312447e-02, 1.00000000e+00,\n",
       "         9.97992252e-01, 9.95912551e-01, 1.15546278e-02, 3.29032167e-02,\n",
       "         3.84666587e-03, 2.00214433e-03, 0.00000000e+00, 5.41933099e-04,\n",
       "         2.66566835e-03, 9.98619999e-01, 0.00000000e+00, 9.98535732e-01,\n",
       "         9.84069216e-01, 2.64867519e-03, 4.64949668e-03, 9.89167525e-01,\n",
       "         1.00000000e+00, 1.00000000e+00, 3.49724522e-01, 1.84637757e-03,\n",
       "         1.00000000e+00, 9.97820652e-01, 9.98601547e-01, 2.27082713e-03,\n",
       "         9.79236957e-01, 9.99261299e-01, 1.79197099e-02, 9.85591150e-01,\n",
       "         9.94974668e-01, 2.80426230e-03, 0.00000000e+00, 9.88355143e-01,\n",
       "         1.64544223e-04, 9.98752764e-01, 7.21309205e-03, 1.00000000e+00],\n",
       "        [9.84156511e-01, 4.28189075e-03, 9.82822530e-01, 3.08972237e-03,\n",
       "         9.93191465e-01, 0.00000000e+00, 9.96822673e-01, 9.96007578e-01,\n",
       "         1.61302171e-03, 0.00000000e+00, 9.98995473e-01, 1.48909493e-03,\n",
       "         9.68318881e-01, 9.95725306e-01, 9.86835453e-01, 9.98946820e-01,\n",
       "         1.00000000e+00, 4.32970244e-04, 9.99248881e-01, 9.95026991e-01,\n",
       "         1.70637494e-03, 9.94809115e-01, 6.24287746e-03, 9.98382682e-01,\n",
       "         2.94939438e-03, 1.00000000e+00, 9.97682192e-01, 9.95308473e-01,\n",
       "         9.83840522e-01, 9.80294628e-01, 9.94958645e-01, 9.93263061e-01,\n",
       "         2.07541355e-03, 0.00000000e+00, 1.80949387e-03, 2.99305559e-03,\n",
       "         5.40134682e-03, 9.98431727e-01, 9.44858775e-01, 9.94061174e-01,\n",
       "         1.67238536e-03, 9.96456073e-01, 1.33404977e-02, 9.77960449e-01,\n",
       "         8.90113894e-01, 1.91448583e-02, 9.83312902e-01, 9.96647546e-01,\n",
       "         9.89554838e-01, 9.77684654e-01, 9.79367986e-01, 9.91155560e-01,\n",
       "         9.95500592e-01, 9.81008251e-01, 4.53059966e-04, 9.78145722e-01,\n",
       "         0.00000000e+00, 9.83291697e-01, 9.84451924e-01, 1.42292420e-03,\n",
       "         9.95899555e-01, 9.91641637e-01, 4.22421228e-04, 3.76403622e-03,\n",
       "         3.75860585e-03, 6.30951953e-01, 3.94203274e-04, 9.99014926e-01,\n",
       "         4.68021817e-03, 2.02452263e-02, 9.98474172e-01, 3.38599736e-03,\n",
       "         9.58581576e-01, 9.96572753e-01, 9.96143322e-01, 9.97710779e-01,\n",
       "         9.72156815e-01, 5.79017298e-01, 9.98573354e-01, 9.94787052e-01,\n",
       "         9.94123253e-01, 9.92013363e-01, 9.97370440e-01, 9.93277466e-01,\n",
       "         9.93716468e-01, 9.94492524e-01, 9.79615772e-01, 9.96887491e-01,\n",
       "         2.52057354e-03, 9.56860189e-01, 6.07721086e-03, 9.99179269e-01,\n",
       "         3.60788227e-03, 9.93341946e-01, 9.97156867e-01, 9.99773085e-01,\n",
       "         5.10848037e-03, 9.83726486e-01, 9.94432630e-01, 9.99596864e-01]]),\n",
       " 1: array([], shape=(0, 100), dtype=float64),\n",
       " 2: array([], shape=(0, 100), dtype=float64),\n",
       " 3: array([], shape=(0, 100), dtype=float64),\n",
       " 4: array([], shape=(0, 100), dtype=float64),\n",
       " 5: array([], shape=(0, 100), dtype=float64)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
