{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9a695e9b-4cc2-43f3-be69-c04cb99dec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TextEmbedder:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def embed(self, text_list):\n",
    "        \"\"\"\n",
    "        Converts a list of text paragraphs into numerical embeddings using TF-IDF.\n",
    "        Parameters:\n",
    "        - text_list (list): List of text paragraphs.\n",
    "        Returns:\n",
    "        - numpy.ndarray: Array of embeddings with shape (n_samples, n_features).\n",
    "        \"\"\"\n",
    "        return self.vectorizer.fit_transform(text_list).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4bdf83e5-b050-461b-97ca-778035c66d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from kneed import KneeLocator\n",
    "\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, max_k=10, max_iter=300, tol=1e-4):\n",
    "        \"\"\"\n",
    "        KMeans clustering with methods for finding the optimal number of clusters using Silhouette Coefficient and Elbow Method.\n",
    "\n",
    "        Parameters:\n",
    "        - max_k (int): Maximum number of clusters to test.\n",
    "        - max_iter (int): Maximum number of iterations for KMeans.\n",
    "        - tol (float): Tolerance for convergence.\n",
    "        \"\"\"\n",
    "        self.max_k = max_k\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.centroids = None\n",
    "        self.labels = None\n",
    "        self.n_clusters = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the K-Means model to the data with the optimal number of clusters.\n",
    "        Returns:\n",
    "        - tuple: (centroids, optimal_k)\n",
    "        \"\"\"\n",
    "        optimal_k = self.find_optimal_k(X)\n",
    "        self.n_clusters = optimal_k\n",
    "        self._fit_kmeans(X, optimal_k)\n",
    "        return self.centroids, optimal_k\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict cluster labels for the data points.\n",
    "        \"\"\"\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "    def find_optimal_k(self, X):\n",
    "        \"\"\"\n",
    "        Find the optimal number of clusters using Silhouette Coefficient and Elbow Method.\n",
    "\n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): The dataset to cluster.\n",
    "\n",
    "        Returns:\n",
    "        - int: Optimal number of clusters.\n",
    "        \"\"\"\n",
    "        silhouette_k = self._find_optimal_k_silhouette(X)\n",
    "        elbow_k = self._find_optimal_k_elbow(X)\n",
    "        self.n_clusters = max(silhouette_k, elbow_k)\n",
    "        return self.n_clusters\n",
    "\n",
    "    def _find_optimal_k_silhouette(self, X):\n",
    "        \"\"\"\n",
    "        Find the optimal number of clusters using the Silhouette Coefficient.\n",
    "        \"\"\"\n",
    "        best_k = 2\n",
    "        best_score = -1\n",
    "        for k in range(2, self.max_k + 1):\n",
    "            self._fit_kmeans(X, k)\n",
    "            score = silhouette_score(X, self.labels)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_k = k\n",
    "        return best_k\n",
    "\n",
    "    def _find_optimal_k_elbow(self, X):\n",
    "        \"\"\"\n",
    "        Find the optimal number of clusters using the Elbow Method.\n",
    "        \"\"\"\n",
    "        distortions = []\n",
    "        for k in range(1, self.max_k + 1):\n",
    "            self._fit_kmeans(X, k)\n",
    "            distortion = np.sum(np.min(np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2), axis=1) ** 2)\n",
    "            distortions.append(distortion)\n",
    "\n",
    "        kneedle = KneeLocator(range(1, len(distortions) + 1), distortions, curve=\"convex\", direction=\"decreasing\")\n",
    "        return kneedle.knee if kneedle.knee else self._fallback_elbow(distortions)\n",
    "\n",
    "    def _fallback_elbow(self, distortions):\n",
    "        \"\"\"\n",
    "        Fallback mechanism to select k if KneeLocator fails.\n",
    "        \"\"\"\n",
    "        gradients = np.gradient(distortions)\n",
    "        second_derivative = np.gradient(gradients)\n",
    "        return np.argmin(second_derivative[1:]) + 2  # Adding 2 to adjust for zero-based index and ignore k=1\n",
    "\n",
    "    def _fit_kmeans(self, X, n_clusters):\n",
    "        \"\"\"\n",
    "        Internal method to fit K-Means with a given number of clusters.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.centroids = X[np.random.choice(n_samples, n_clusters, replace=False)]\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            labels = self._assign_clusters(X)\n",
    "            new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(n_clusters)])\n",
    "            if np.all(np.abs(new_centroids - self.centroids) < self.tol):\n",
    "                break\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "        self.labels = self._assign_clusters(X)\n",
    "\n",
    "    def _assign_clusters(self, X):\n",
    "        \"\"\"\n",
    "        Assign data points to the nearest centroid.\n",
    "        \"\"\"\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
    "        return np.argmin(distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b3a98e1d-f1a7-4526-9032-3178843c188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class GMM:\n",
    "    def __init__(self, n_components, max_iter=100, tol=1e-3, reg_covar=1e-6, initial_means=None):\n",
    "        \"\"\"\n",
    "        Gaussian Mixture Model for clustering.\n",
    "\n",
    "        Parameters:\n",
    "        - n_components (int): Number of components (clusters).\n",
    "        - max_iter (int): Maximum number of iterations for the EM algorithm.\n",
    "        - tol (float): Convergence tolerance.\n",
    "        - reg_covar (float): Regularization added to the covariance matrices.\n",
    "        - initial_means (numpy.ndarray): Initial mean values for the clusters.\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.reg_covar = reg_covar\n",
    "        self.initial_means = initial_means\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the GMM model to the data.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # Initialize means with KMeans results if provided\n",
    "        if self.initial_means is not None:\n",
    "            self.means = self.initial_means\n",
    "        else:\n",
    "            self.means = X[np.random.choice(n_samples, self.n_components, replace=False)]\n",
    "        self.covariances = np.zeros((self.n_components, n_features, n_features))\n",
    "        self.weights = np.full(self.n_components, 1 / self.n_components)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            responsibilities = self._e_step(X)\n",
    "            self._m_step(X, responsibilities)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the cluster labels for the data points.\n",
    "        \"\"\"\n",
    "        probabilities = self._e_step(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    def _e_step(self, X):\n",
    "        \"\"\"\n",
    "        E-step: Compute responsibilities for each data point and cluster.\n",
    "        \"\"\"\n",
    "        probabilities = np.zeros((X.shape[0], self.n_components))\n",
    "        for k in range(self.n_components):\n",
    "            cov_matrix = self.covariances[k] + np.eye(X.shape[1]) * self.reg_covar  # Regularize covariance\n",
    "            pdf_values = multivariate_normal(self.means[k], cov_matrix).pdf(X)\n",
    "            probabilities[:, k] = self.weights[k] * np.clip(pdf_values, 1e-300, 1e+300)\n",
    "        prob_sum = probabilities.sum(axis=1, keepdims=True)\n",
    "        prob_sum[prob_sum == 0] = 1  # Prevent division by zero\n",
    "        probabilities /= prob_sum\n",
    "        return probabilities\n",
    "\n",
    "    def _m_step(self, X, responsibilities):\n",
    "        \"\"\"\n",
    "        M-step: Update the GMM parameters.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        for k in range(self.n_components):\n",
    "            responsibility = responsibilities[:, k]\n",
    "            total_responsibility = responsibility.sum()\n",
    "            self.means[k] = (responsibility[:, np.newaxis] * X).sum(axis=0) / total_responsibility\n",
    "\n",
    "            # Compute covariance matrix based on the points assigned to the cluster\n",
    "            diff = X - self.means[k]\n",
    "            self.covariances[k] = (responsibility[:, np.newaxis, np.newaxis] * diff[:, :, np.newaxis] * diff[:, np.newaxis, :]).sum(axis=0) / total_responsibility\n",
    "\n",
    "            # Regularize covariance matrix to avoid singularity issues\n",
    "            self.covariances[k] += np.eye(X.shape[1]) * self.reg_covar\n",
    "\n",
    "            # Compute the weight for each component\n",
    "            self.weights[k] = total_responsibility / n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "09c700b9-ac35-44c4-aca8-dee845a89695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubtClustering:\n",
    "    def __init__(self, max_k=10):\n",
    "        self.max_k = max_k\n",
    "\n",
    "    def cluster(self, text_paragraphs):\n",
    "        \"\"\"\n",
    "        Cluster the text paragraphs using an ensemble of KMeans and GMM.\n",
    "\n",
    "        Parameters:\n",
    "        - text_paragraphs (list of str): Text paragraphs to be clustered.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary where keys are cluster labels, and values are lists of text paragraphs in each cluster.\n",
    "        \"\"\"\n",
    "        # Step 1: Convert text to embeddings\n",
    "        embedder = TextEmbedder()\n",
    "        embeddings = embedder.embed(text_paragraphs)\n",
    "\n",
    "        # Step 2: Find optimal K using Elbow Method and Perform K-means to perform Clusters\n",
    "        kmeans = KMeans(max_k=10)\n",
    "        centroids, optimal_k = kmeans.fit(X)\n",
    "        # Step 4: Refine using GMM with KMeans centroids as initial means\n",
    "        gmm = GMM(n_components=optimal_k if optimal_k is not None else 4, initial_means=centroids)  # Pass KMeans centroids to GMM)\n",
    "        gmm.fit(embeddings)\n",
    "        gmm_labels = gmm.predict(embeddings)\n",
    "\n",
    "        # Organize clusters into a dictionary\n",
    "        clusters = {}\n",
    "        for i, label in enumerate(gmm_labels):\n",
    "            if label not in clusters:\n",
    "                clusters[label] = []\n",
    "            clusters[label].append(text_paragraphs[i])\n",
    "\n",
    "        return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3677255c-4cd9-4a79-aee7-d1154f4e62cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension mismatch: array 'cov' is of shape (46, 46), but 'mean' is a vector of length 384.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 14\u001b[0m\n\u001b[0;32m      2\u001b[0m text_paragraphs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is AI and how does it differ from machine learning?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the applications of deep learning in healthcare?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the ethical considerations of AI in decision-making systems?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m ]\n\u001b[0;32m     13\u001b[0m clustering \u001b[38;5;241m=\u001b[39m DoubtClustering(max_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m clusters \u001b[38;5;241m=\u001b[39m \u001b[43mclustering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_paragraphs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cluster_id, texts \u001b[38;5;129;01min\u001b[39;00m clusters\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCluster \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[136], line 24\u001b[0m, in \u001b[0;36mDoubtClustering.cluster\u001b[1;34m(self, text_paragraphs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Step 4: Refine using GMM with KMeans centroids as initial means\u001b[39;00m\n\u001b[0;32m     23\u001b[0m gmm \u001b[38;5;241m=\u001b[39m GMM(n_components\u001b[38;5;241m=\u001b[39moptimal_k \u001b[38;5;28;01mif\u001b[39;00m optimal_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m4\u001b[39m, initial_means\u001b[38;5;241m=\u001b[39mcentroids)  \u001b[38;5;66;03m# Pass KMeans centroids to GMM)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mgmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m gmm_labels \u001b[38;5;241m=\u001b[39m gmm\u001b[38;5;241m.\u001b[39mpredict(embeddings)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Organize clusters into a dictionary\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[135], line 36\u001b[0m, in \u001b[0;36mGMM.fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[1;32m---> 36\u001b[0m     responsibilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_m_step(X, responsibilities)\n",
      "Cell \u001b[1;32mIn[135], line 53\u001b[0m, in \u001b[0;36mGMM._e_step\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components):\n\u001b[0;32m     52\u001b[0m     cov_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariances[k] \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39meye(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_covar  \u001b[38;5;66;03m# Regularize covariance\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     pdf_values \u001b[38;5;241m=\u001b[39m \u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeans\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_matrix\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpdf(X)\n\u001b[0;32m     54\u001b[0m     probabilities[:, k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[k] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(pdf_values, \u001b[38;5;241m1e-300\u001b[39m, \u001b[38;5;241m1e+300\u001b[39m)\n\u001b[0;32m     55\u001b[0m prob_sum \u001b[38;5;241m=\u001b[39m probabilities\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\stats\\_multivariate.py:397\u001b[0m, in \u001b[0;36mmultivariate_normal_gen.__call__\u001b[1;34m(self, mean, cov, allow_singular, seed)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cov\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, allow_singular\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    393\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a frozen multivariate normal distribution.\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \n\u001b[0;32m    395\u001b[0m \u001b[38;5;124;03m    See `multivariate_normal_frozen` for more information.\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmultivariate_normal_frozen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mallow_singular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_singular\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\stats\\_multivariate.py:903\u001b[0m, in \u001b[0;36mmultivariate_normal_frozen.__init__\u001b[1;34m(self, mean, cov, allow_singular, seed, maxpts, abseps, releps)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a frozen multivariate normal distribution.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    899\u001b[0m \n\u001b[0;32m    900\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m \u001b[38;5;66;03m# numpy/numpydoc#87  # noqa: E501\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dist \u001b[38;5;241m=\u001b[39m multivariate_normal_gen(seed)\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov_object \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 903\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_singular\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_singular \u001b[38;5;241m=\u001b[39m allow_singular \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov_object\u001b[38;5;241m.\u001b[39m_allow_singular\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m maxpts:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\stats\\_multivariate.py:414\u001b[0m, in \u001b[0;36mmultivariate_normal_gen._process_parameters\u001b[1;34m(self, mean, cov, allow_singular)\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_parameters_Covariance(mean, cov)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# Before `Covariance` classes were introduced,\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;66;03m# `multivariate_normal` accepted plain arrays as `cov` and used the\u001b[39;00m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;66;03m# following input validation. To avoid disturbing the behavior of\u001b[39;00m\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;66;03m# `multivariate_normal` when plain arrays are used, we use the\u001b[39;00m\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;66;03m# original input validation here.\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m     dim, mean, cov \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_parameters_psd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;66;03m# After input validation, some methods then processed the arrays\u001b[39;00m\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# with a `_PSD` object and used that to perform computation.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;66;03m# To avoid branching statements in each method depending on whether\u001b[39;00m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;66;03m# `cov` is an array or `Covariance` object, we always process the\u001b[39;00m\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;66;03m# array with `_PSD`, and then use wrapper that satisfies the\u001b[39;00m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;66;03m# `Covariance` interface, `CovViaPSD`.\u001b[39;00m\n\u001b[0;32m    421\u001b[0m     psd \u001b[38;5;241m=\u001b[39m _PSD(cov, allow_singular\u001b[38;5;241m=\u001b[39mallow_singular)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\stats\\_multivariate.py:486\u001b[0m, in \u001b[0;36mmultivariate_normal_gen._process_parameters_psd\u001b[1;34m(self, dim, mean, cov)\u001b[0m\n\u001b[0;32m    483\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimension mismatch: array \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcov\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is of shape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    484\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is a vector of length \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    485\u001b[0m         msg \u001b[38;5;241m=\u001b[39m msg \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mstr\u001b[39m(cov\u001b[38;5;241m.\u001b[39mshape), \u001b[38;5;28mlen\u001b[39m(mean))\n\u001b[1;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cov\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArray \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcov\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be at most two-dimensional,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    489\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but cov.ndim = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m cov\u001b[38;5;241m.\u001b[39mndim)\n",
      "\u001b[1;31mValueError\u001b[0m: Dimension mismatch: array 'cov' is of shape (46, 46), but 'mean' is a vector of length 384."
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text_paragraphs = [\n",
    "    \"What is AI and how does it differ from machine learning?\",\n",
    "    \"What are the applications of deep learning in healthcare?\",\n",
    "    \"Explain the importance of network security protocols.\",\n",
    "    \"What is the difference between supervised and unsupervised learning?\",\n",
    "    \"How do databases handle concurrent users efficiently?\",\n",
    "    \"What are the challenges in developing blockchain technologies?\",\n",
    "    \"How does PCA help in dimensionality reduction?\",\n",
    "    \"What are the ethical considerations of AI in decision-making systems?\"\n",
    "]\n",
    "\n",
    "clustering = DoubtClustering(max_k=10)\n",
    "clusters = clustering.cluster(text_paragraphs)\n",
    "\n",
    "for cluster_id, texts in clusters.items():\n",
    "    print(f\"Cluster {cluster_id}:\")\n",
    "    for text in texts:\n",
    "        print(f\"  - {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b2cb0c14-ce9f-487e-b1bd-bf4089b383bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paragraphs = [\n",
    "    # Machine Learning and AI\n",
    "    \"Why do we need softmax, and how is it different from sigmoid in multi-class classification problems?\",\n",
    "    \"Explain the Gaussian Mixture Model algorithm and its applications in anomaly detection.\",\n",
    "    \"What are supervised learning techniques, and how do they compare with semi-supervised learning in terms of labeled data requirements?\",\n",
    "    \"How does rule-based AI work, and can it handle dynamic environments effectively?\",\n",
    "    \"What are the common challenges faced when training deep learning models with imbalanced datasets?\",\n",
    "    \"Can you explain the concept of overfitting and how dropout regularization helps to mitigate it?\",\n",
    "    \"How do transformer models handle long-term dependencies compared to recurrent neural networks?\",\n",
    "    \"What is the role of the attention mechanism in natural language processing tasks?\",\n",
    "    \"How does gradient clipping prevent exploding gradients during backpropagation in deep networks?\",\n",
    "    \"What are the ethical considerations when using AI in decision-making systems, such as hiring or loan approvals?\",\n",
    "    \n",
    "    # Networking and Web Technologies\n",
    "    \"What are network sockets, and how do they facilitate communication between distributed systems?\",\n",
    "    \"What is a web server, and how does it differ from an application server in terms of functionality?\",\n",
    "    \"What is the difference between TCP and UDP, and how do they affect data transmission reliability?\",\n",
    "    \"How does DNS caching improve website load times, and what are its potential downsides?\",\n",
    "    \"Explain the role of SSL/TLS in securing HTTP connections.\",\n",
    "    \"What are Content Delivery Networks (CDNs), and how do they handle geographically distributed traffic?\",\n",
    "    \"How does NAT (Network Address Translation) work, and what are its implications for IPv4 addresses?\",\n",
    "    \"What are RESTful APIs, and how do they differ from GraphQL in terms of data fetching flexibility?\",\n",
    "    \"How does a firewall filter incoming and outgoing traffic, and what are common configurations for web security?\",\n",
    "    \"What are the advantages of using WebSocket over HTTP for real-time applications?\",\n",
    "]\n",
    "\n",
    "test_par_2 = [\n",
    "    # Databases and Data Management\n",
    "    \"How do we know if an index for a database is useful, and what metrics can we use to assess its performance?\",\n",
    "    \"What is the difference between a primary key and a foreign key, and how do they maintain relational integrity?\",\n",
    "    \"What is a surrogate key, and why might it be preferred over natural keys in database design?\",\n",
    "    \"What are the trade-offs between normalization and denormalization in relational databases?\",\n",
    "    \"How does a query optimizer decide on the best execution plan for SQL queries?\",\n",
    "    \"What is the CAP theorem, and how does it apply to distributed databases like Cassandra or MongoDB?\",\n",
    "    \"How does ACID compliance ensure reliability in transaction processing?\",\n",
    "    \"What are materialized views, and how do they differ from regular views in terms of data storage?\",\n",
    "    \"Explain the differences between row-oriented and column-oriented databases and their respective use cases.\",\n",
    "    \"How does sharding improve database scalability, and what are the challenges associated with it?\",\n",
    "\n",
    "    # Mathematics and Theoretical Concepts\n",
    "    \"Who discovered vector spaces, and how are they foundational to linear algebra?\",\n",
    "    \"What are eigenvalues and eigenvectors, and why are they significant in dimensionality reduction techniques like PCA?\",\n",
    "    \"How do you compute the determinant of a matrix, and what does its value signify in terms of invertibility?\",\n",
    "    \"What is the difference between a discrete and a continuous probability distribution, and where are each applied?\",\n",
    "    \"Can you explain the concept of Bayesian inference and its applications in modern statistics?\",\n",
    "    \"What is the difference between convex and non-convex optimization problems, and why are the latter harder to solve?\",\n",
    "    \"How do Markov Chains model stochastic processes, and where are they applied in machine learning?\",\n",
    "    \"What is the significance of the Fourier Transform in signal processing and image analysis?\",\n",
    "    \"How do graph theory concepts like spanning trees and shortest paths relate to computer networking?\",\n",
    "    \"What is the role of the Laplace Transform in solving differential equations?\",\n",
    "\n",
    "    # Miscellaneous Topics\n",
    "    \"What is the Turing Test, and how effective is it in assessing machine intelligence?\",\n",
    "    \"What are quantum computers, and how do they differ fundamentally from classical computers?\",\n",
    "    \"What are the different types of memory (cache, RAM, ROM), and how do they interact with the CPU?\",\n",
    "    \"What are Bloom Filters, and how are they used in applications like spell checkers and caching?\",\n",
    "    \"How does a blockchain achieve consensus, and what are the differences between Proof of Work and Proof of Stake?\",\n",
    "    \"What is the difference between symmetric and asymmetric encryption, and when would you use each?\",\n",
    "    \"What are software design patterns, and why is the Singleton pattern considered controversial?\",\n",
    "    \"How does garbage collection work in programming languages like Java, and what are the trade-offs involved?\",\n",
    "    \"What is a finite state machine, and how are they used in designing software systems?\",\n",
    "    \"How does Agile methodology differ from Waterfall in software development workflows?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b35e789c-2a5f-462c-b661-81bc9c4c231d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with text_paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rijusmit\\AppData\\Local\\Temp\\ipykernel_34184\\2735809285.py:16: RuntimeWarning: Mean of empty slice.\n",
      "  new_centroids = np.array([X[self.labels == k].mean(axis=0) for k in range(self.n_clusters)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters Formed: 2\n",
      "\n",
      "Cluster 1 (4 items):\n",
      "  - What is AI and how does it differ from machine learning?\n",
      "  - What are the applications of deep learning in healthcare?\n",
      "  - Explain the importance of network security protocols.\n",
      "  - What are the ethical considerations of AI in decision-making systems?\n",
      "\n",
      "Cluster 0 (4 items):\n",
      "  - What is the difference between supervised and unsupervised learning?\n",
      "  - How do databases handle concurrent users efficiently?\n",
      "  - What are the challenges in developing blockchain technologies?\n",
      "  - How does PCA help in dimensionality reduction?\n"
     ]
    }
   ],
   "source": [
    "# Define a testing function\n",
    "def test_clustering(paragraphs, max_k=10):\n",
    "    \"\"\"\n",
    "    Tests the clustering implementation with the given paragraphs.\n",
    "    \n",
    "    Parameters:\n",
    "    - paragraphs (list): List of text paragraphs to cluster.\n",
    "    - max_k (int): Maximum number of clusters for the elbow method.\n",
    "    \"\"\"\n",
    "    clustering = DoubtClustering(max_k=max_k)\n",
    "    clusters = clustering.cluster(paragraphs)\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"Number of Clusters Formed:\", len(clusters))\n",
    "    for cluster_id, cluster_paragraphs in clusters.items():\n",
    "        print(f\"\\nCluster {cluster_id} ({len(cluster_paragraphs)} items):\")\n",
    "        for paragraph in cluster_paragraphs:\n",
    "            print(f\"  - {paragraph}\")\n",
    "\n",
    "print(\"\\nTesting with text_paragraphs...\")\n",
    "test_clustering(text_paragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "daaad164-a4e1-4cf7-a9fc-17b6ed0d6bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rijusmit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Option 1: TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust features for scalability\n",
    "X = vectorizer.fit_transform(text_paragraphs).toarray()\n",
    "\n",
    "# Option 2: Sentence Embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight transformer for embeddings\n",
    "X = model.encode(text_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6774a1c-f096-4cb9-9aed-a53e7d96fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d46a81a-f89d-4e13-b8e9-86f3061008e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
