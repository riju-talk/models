{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a695e9b-4cc2-43f3-be69-c04cb99dec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TextEmbedder:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def embed(self, text_list):\n",
    "        \"\"\"\n",
    "        Converts a list of text paragraphs into numerical embeddings using TF-IDF.\n",
    "        Parameters:\n",
    "        - text_list (list): List of text paragraphs.\n",
    "        Returns:\n",
    "        - numpy.ndarray: Array of embeddings with shape (n_samples, n_features).\n",
    "        \"\"\"\n",
    "        return self.vectorizer.fit_transform(text_list).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4bdf83e5-b050-461b-97ca-778035c66d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from kneed import KneeLocator\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, max_k=10, max_iter=300, tol=1e-4):\n",
    "        \"\"\"\n",
    "        KMeans clustering with methods for finding the optimal number of clusters.\n",
    "        \n",
    "        Parameters:\n",
    "        - max_k (int): Maximum number of clusters to test\n",
    "        - max_iter (int): Maximum number of iterations for KMeans\n",
    "        - tol (float): Tolerance for convergence\n",
    "        \"\"\"\n",
    "        self.max_k = max_k\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.centroids = None\n",
    "        self.labels = None\n",
    "        self.n_clusters = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the K-Means model to the data with the optimal number of clusters.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input data of shape (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        - tuple: (centroids, optimal_k)\n",
    "        \"\"\"\n",
    "        # Validate input\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Adjust max_k to be no larger than n_samples - 1\n",
    "        self.max_k = min(self.max_k, n_samples - 1)\n",
    "        \n",
    "        # Find optimal k\n",
    "        optimal_k = self.find_optimal_k(X)\n",
    "        self.n_clusters = optimal_k\n",
    "        \n",
    "        # Fit final model with optimal k\n",
    "        self._fit_kmeans(X, optimal_k)\n",
    "        return self.centroids, optimal_k\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict cluster labels for the data points.\n",
    "        \"\"\"\n",
    "        if self.centroids is None:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "        \n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "            \n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
    "        return np.argmin(distances, axis=1)\n",
    "    \n",
    "    def find_optimal_k(self, X):\n",
    "        \"\"\"\n",
    "        Find the optimal number of clusters using both Silhouette Coefficient \n",
    "        and Elbow Method.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        max_possible_k = min(self.max_k, n_samples - 1)\n",
    "        \n",
    "        if max_possible_k < 2:\n",
    "            return 2  # Minimum number of clusters\n",
    "            \n",
    "        silhouette_k = self._find_optimal_k_silhouette(X, max_possible_k)\n",
    "        elbow_k = self._find_optimal_k_elbow(X, max_possible_k)\n",
    "        \n",
    "        # Use the average of both methods, rounded to nearest integer\n",
    "        optimal_k = max(2, min(max_possible_k, round((silhouette_k + elbow_k) / 2)))\n",
    "        return optimal_k\n",
    "    \n",
    "    def _find_optimal_k_silhouette(self, X, max_k):\n",
    "        \"\"\"\n",
    "        Find optimal k using Silhouette Coefficient.\n",
    "        \"\"\"\n",
    "        best_k = 2\n",
    "        best_score = -1\n",
    "        \n",
    "        for k in range(2, max_k + 1):\n",
    "            self._fit_kmeans(X, k)\n",
    "            if len(np.unique(self.labels)) > 1:\n",
    "                try:\n",
    "                    score = silhouette_score(X, self.labels)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_k = k\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                    \n",
    "        return best_k\n",
    "    \n",
    "    def _find_optimal_k_elbow(self, X, max_k):\n",
    "        \"\"\"\n",
    "        Find optimal k using Elbow Method.\n",
    "        \"\"\"\n",
    "        distortions = []\n",
    "        k_values = range(1, max_k + 1)\n",
    "        \n",
    "        for k in k_values:\n",
    "            self._fit_kmeans(X, k)\n",
    "            distortion = np.sum(np.min(np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2), axis=1) ** 2)\n",
    "            distortions.append(distortion)\n",
    "        \n",
    "        try:\n",
    "            kneedle = KneeLocator(\n",
    "                k_values, \n",
    "                distortions, \n",
    "                curve=\"convex\", \n",
    "                direction=\"decreasing\",\n",
    "                online=True\n",
    "            )\n",
    "            if kneedle.knee is not None:\n",
    "                return kneedle.knee\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "        return self._fallback_elbow(distortions)\n",
    "    \n",
    "    def _fallback_elbow(self, distortions):\n",
    "        \"\"\"\n",
    "        Fallback method to find elbow point using second derivative.\n",
    "        \"\"\"\n",
    "        if len(distortions) < 3:\n",
    "            return 2\n",
    "            \n",
    "        # Calculate second derivative\n",
    "        gradients = np.gradient(distortions)\n",
    "        second_derivative = np.gradient(gradients)\n",
    "        \n",
    "        # Find the point of maximum curvature\n",
    "        elbow_index = np.argmax(np.abs(second_derivative[1:-1])) + 1\n",
    "        return elbow_index + 1  # Add 1 because k starts from 1\n",
    "    \n",
    "    def _fit_kmeans(self, X, n_clusters):\n",
    "        \"\"\"\n",
    "        Internal method to fit KMeans with a given number of clusters.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Initialize centroids using k-means++\n",
    "        self.centroids = self._kmeans_plus_plus_init(X, n_clusters)\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            old_centroids = self.centroids.copy()\n",
    "            \n",
    "            # Assign clusters\n",
    "            self.labels = self._assign_clusters(X)\n",
    "            \n",
    "            # Update centroids\n",
    "            for k in range(n_clusters):\n",
    "                if np.sum(self.labels == k) > 0:  # Only update if cluster has points\n",
    "                    self.centroids[k] = X[self.labels == k].mean(axis=0)\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.all(np.abs(old_centroids - self.centroids) < self.tol):\n",
    "                break\n",
    "                \n",
    "    def _kmeans_plus_plus_init(self, X, n_clusters):\n",
    "        \"\"\"\n",
    "        Initialize centroids using k-means++ algorithm.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        centroids = [X[np.random.randint(n_samples)]]\n",
    "        \n",
    "        for _ in range(1, n_clusters):\n",
    "            distances = np.min([np.linalg.norm(X - c, axis=1) ** 2 for c in centroids], axis=0)\n",
    "            probs = distances / distances.sum()\n",
    "            cumprobs = np.cumsum(probs)\n",
    "            r = np.random.random()\n",
    "            \n",
    "            for j, p in enumerate(cumprobs):\n",
    "                if r < p:\n",
    "                    centroids.append(X[j])\n",
    "                    break\n",
    "                    \n",
    "        return np.array(centroids)\n",
    "    \n",
    "    def _assign_clusters(self, X):\n",
    "        \"\"\"\n",
    "        Assign data points to nearest centroid.\n",
    "        \"\"\"\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
    "        return np.argmin(distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b3a98e1d-f1a7-4526-9032-3178843c188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class GMM:\n",
    "    def __init__(self, n_components, max_iter=100, tol=1e-3, reg_covar=1e-6, initial_means=None):\n",
    "        \"\"\"\n",
    "        Gaussian Mixture Model for clustering.\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.reg_covar = reg_covar\n",
    "        self.initial_means = initial_means\n",
    "        self.means = None\n",
    "        self.covariances = None\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the GMM model to the data.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize means\n",
    "        if self.initial_means is not None:\n",
    "            self.means = self.initial_means\n",
    "        else:\n",
    "            indices = np.random.choice(n_samples, self.n_components, replace=False)\n",
    "            self.means = X[indices]\n",
    "\n",
    "        # Initialize covariances with data-driven values\n",
    "        self.covariances = np.array([\n",
    "            np.cov(X.T) + np.eye(n_features) * self.reg_covar \n",
    "            for _ in range(self.n_components)\n",
    "        ])\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weights = np.ones(self.n_components) / self.n_components\n",
    "        \n",
    "        prev_log_likelihood = None\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            try:\n",
    "                # E-step\n",
    "                responsibilities = self._e_step(X)\n",
    "                \n",
    "                # M-step\n",
    "                self._m_step(X, responsibilities)\n",
    "                \n",
    "                # Check convergence\n",
    "                log_likelihood = self._compute_log_likelihood(X)\n",
    "                if prev_log_likelihood is not None:\n",
    "                    change = abs(log_likelihood - prev_log_likelihood)\n",
    "                    if change < self.tol:\n",
    "                        break\n",
    "                prev_log_likelihood = log_likelihood\n",
    "                \n",
    "            except np.linalg.LinAlgError:\n",
    "                self.reg_covar *= 10\n",
    "                continue\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict cluster assignments.\n",
    "        \"\"\"\n",
    "        probabilities = self._e_step(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    def _e_step(self, X):\n",
    "        \"\"\"\n",
    "        E-step: Compute responsibilities.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        responsibilities = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            try:\n",
    "                # Add regularization to ensure positive definiteness\n",
    "                cov = self.covariances[k] + np.eye(X.shape[1]) * self.reg_covar\n",
    "                \n",
    "                # Ensure symmetry\n",
    "                cov = (cov + cov.T) / 2\n",
    "                \n",
    "                rv = multivariate_normal(\n",
    "                    mean=self.means[k],\n",
    "                    cov=cov,\n",
    "                    allow_singular=True\n",
    "                )\n",
    "                density = rv.pdf(X)\n",
    "                responsibilities[:, k] = self.weights[k] * np.maximum(density, np.finfo(float).tiny)\n",
    "            except:\n",
    "                responsibilities[:, k] = np.finfo(float).tiny\n",
    "        \n",
    "        # Normalize responsibilities\n",
    "        row_sums = responsibilities.sum(axis=1)\n",
    "        responsibilities /= row_sums[:, np.newaxis]\n",
    "        \n",
    "        return responsibilities\n",
    "\n",
    "    def _m_step(self, X, responsibilities):\n",
    "        \"\"\"\n",
    "        M-step: Update parameters.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Update weights (mixing coefficients)\n",
    "        Nk = responsibilities.sum(axis=0)\n",
    "        self.weights = Nk / n_samples\n",
    "        \n",
    "        # Update means\n",
    "        self.means = np.dot(responsibilities.T, X) / Nk[:, np.newaxis]\n",
    "        \n",
    "        # Update covariances\n",
    "        for k in range(self.n_components):\n",
    "            diff = X - self.means[k]\n",
    "            weighted_diff = responsibilities[:, k:k+1] * diff\n",
    "            cov = np.dot(weighted_diff.T, diff) / Nk[k]\n",
    "            \n",
    "            # Ensure covariance matrix is well-conditioned\n",
    "            min_eig = np.linalg.eigvalsh(cov).min()\n",
    "            if min_eig < self.reg_covar:\n",
    "                cov += np.eye(n_features) * (self.reg_covar - min_eig)\n",
    "            \n",
    "            self.covariances[k] = cov\n",
    "    \n",
    "    def _compute_log_likelihood(self, X):\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood of the data.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        likelihood = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            cov = self.covariances[k] + np.eye(X.shape[1]) * self.reg_covar\n",
    "            rv = multivariate_normal(mean=self.means[k], cov=cov, allow_singular=True)\n",
    "            likelihood[:, k] = self.weights[k] * rv.pdf(X)\n",
    "        \n",
    "        return np.sum(np.log(np.sum(likelihood, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09c700b9-ac35-44c4-aca8-dee845a89695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubtClustering:\n",
    "    def __init__(self, max_k=10):\n",
    "        self.max_k = max_k\n",
    "\n",
    "    def cluster(self, text_paragraphs):\n",
    "        \"\"\"\n",
    "        Cluster the text paragraphs using an ensemble of KMeans and GMM.\n",
    "\n",
    "        Parameters:\n",
    "        - text_paragraphs (list of str): Text paragraphs to be clustered.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary where keys are cluster labels, and values are lists of text paragraphs in each cluster.\n",
    "        \"\"\"\n",
    "        # Step 1: Convert text to embeddings\n",
    "        embedder = TextEmbedder()\n",
    "        embeddings = embedder.embed(text_paragraphs)\n",
    "\n",
    "        # Step 2: Find optimal K using Elbow Method and Perform K-means to perform Clusters\n",
    "        kmeans = KMeans(max_k=10)\n",
    "        centroids, optimal_k = kmeans.fit(embeddings)\n",
    "        # Step 4: Refine using GMM with KMeans centroids as initial means\n",
    "        gmm = GMM(n_components=optimal_k if optimal_k is not None else 4, initial_means=centroids)  # Pass KMeans centroids to GMM)\n",
    "        gmm.fit(embeddings)\n",
    "        gmm_labels = gmm.predict(embeddings)\n",
    "\n",
    "        # Organize clusters into a dictionary\n",
    "        clusters = {}\n",
    "        for i, label in enumerate(gmm_labels):\n",
    "            if label not in clusters:\n",
    "                clusters[label] = []\n",
    "            clusters[label].append(text_paragraphs[i])\n",
    "\n",
    "        return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3677255c-4cd9-4a79-aee7-d1154f4e62cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "  - What is AI and how does it differ from machine learning?\n",
      "  - What are the ethical considerations of AI in decision-making systems?\n",
      "Cluster 1:\n",
      "  - What are the applications of deep learning in healthcare?\n",
      "  - Explain the importance of network security protocols.\n",
      "  - What is the difference between supervised and unsupervised learning?\n",
      "  - What are the challenges in developing blockchain technologies?\n",
      "Cluster 2:\n",
      "  - How do databases handle concurrent users efficiently?\n",
      "  - How does PCA help in dimensionality reduction?\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text_paragraphs = [\n",
    "    \"What is AI and how does it differ from machine learning?\",\n",
    "    \"What are the applications of deep learning in healthcare?\",\n",
    "    \"Explain the importance of network security protocols.\",\n",
    "    \"What is the difference between supervised and unsupervised learning?\",\n",
    "    \"How do databases handle concurrent users efficiently?\",\n",
    "    \"What are the challenges in developing blockchain technologies?\",\n",
    "    \"How does PCA help in dimensionality reduction?\",\n",
    "    \"What are the ethical considerations of AI in decision-making systems?\"\n",
    "]\n",
    "\n",
    "clustering = DoubtClustering(max_k=10)\n",
    "clusters = clustering.cluster(text_paragraphs)\n",
    "\n",
    "for cluster_id, texts in clusters.items():\n",
    "    print(f\"Cluster {cluster_id}:\")\n",
    "    for text in texts:\n",
    "        print(f\"  - {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b2cb0c14-ce9f-487e-b1bd-bf4089b383bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paragraphs_1 = [\n",
    "    # Machine Learning and AI\"\"\"\n",
    "    \"Why do we need softmax, and how is it different from sigmoid in multi-class classification problems?\",\n",
    "    \"Explain the Gaussian Mixture Model algorithm and its applications in anomaly detection.\",\n",
    "    \"What are supervised learning techniques, and how do they compare with semi-supervised learning in terms of labeled data requirements?\",\n",
    "    \"How does rule-based AI work, and can it handle dynamic environments effectively?\",\n",
    "    \"What are the common challenges faced when training deep learning models with imbalanced datasets?\",\n",
    "    \"Can you explain the concept of overfitting and how dropout regularization helps to mitigate it?\",\n",
    "    \"How do transformer models handle long-term dependencies compared to recurrent neural networks?\",\n",
    "    \"What is the role of the attention mechanism in natural language processing tasks?\",\n",
    "    \"How does gradient clipping prevent exploding gradients during backpropagation in deep networks?\",\n",
    "    \"What are the ethical considerations when using AI in decision-making systems, such as hiring or loan approvals?\"]\n",
    "test_paragraphs_2 = [\n",
    "    \n",
    "    # Networking and Web Technologies\n",
    "    \"What are network sockets, and how do they facilitate communication between distributed systems?\",\n",
    "    \"What is a web server, and how does it differ from an application server in terms of functionality?\",\n",
    "    \"What is the difference between TCP and UDP, and how do they affect data transmission reliability?\",\n",
    "    \"How does DNS caching improve website load times, and what are its potential downsides?\",\n",
    "    \"Explain the role of SSL/TLS in securing HTTP connections.\",\n",
    "    \"What are Content Delivery Networks (CDNs), and how do they handle geographically distributed traffic?\",\n",
    "    \"How does NAT (Network Address Translation) work, and what are its implications for IPv4 addresses?\",\n",
    "    \"What are RESTful APIs, and how do they differ from GraphQL in terms of data fetching flexibility?\",\n",
    "    \"How does a firewall filter incoming and outgoing traffic, and what are common configurations for web security?\",\n",
    "    \"What are the advantages of using WebSocket over HTTP for real-time applications?\",\n",
    "]\n",
    "\n",
    "test_par_2 = [\n",
    "    # Databases and Data Management\n",
    "    \"How do we know if an index for a database is useful, and what metrics can we use to assess its performance?\",\n",
    "    \"What is the difference between a primary key and a foreign key, and how do they maintain relational integrity?\",\n",
    "    \"What is a surrogate key, and why might it be preferred over natural keys in database design?\",\n",
    "    \"What are the trade-offs between normalization and denormalization in relational databases?\",\n",
    "    \"How does a query optimizer decide on the best execution plan for SQL queries?\",\n",
    "    \"What is the CAP theorem, and how does it apply to distributed databases like Cassandra or MongoDB?\",\n",
    "    \"How does ACID compliance ensure reliability in transaction processing?\",\n",
    "    \"What are materialized views, and how do they differ from regular views in terms of data storage?\",\n",
    "    \"Explain the differences between row-oriented and column-oriented databases and their respective use cases.\",\n",
    "    \"How does sharding improve database scalability, and what are the challenges associated with it?\",\n",
    "\n",
    "    # Mathematics and Theoretical Concepts\n",
    "    \"Who discovered vector spaces, and how are they foundational to linear algebra?\",\n",
    "    \"What are eigenvalues and eigenvectors, and why are they significant in dimensionality reduction techniques like PCA?\",\n",
    "    \"How do you compute the determinant of a matrix, and what does its value signify in terms of invertibility?\",\n",
    "    \"What is the difference between a discrete and a continuous probability distribution, and where are each applied?\",\n",
    "    \"Can you explain the concept of Bayesian inference and its applications in modern statistics?\",\n",
    "    \"What is the difference between convex and non-convex optimization problems, and why are the latter harder to solve?\",\n",
    "    \"How do Markov Chains model stochastic processes, and where are they applied in machine learning?\",\n",
    "    \"What is the significance of the Fourier Transform in signal processing and image analysis?\",\n",
    "    \"How do graph theory concepts like spanning trees and shortest paths relate to computer networking?\",\n",
    "    \"What is the role of the Laplace Transform in solving differential equations?\",\n",
    "\n",
    "    # Miscellaneous Topics\n",
    "    \"What is the Turing Test, and how effective is it in assessing machine intelligence?\",\n",
    "    \"What are quantum computers, and how do they differ fundamentally from classical computers?\",\n",
    "    \"What are the different types of memory (cache, RAM, ROM), and how do they interact with the CPU?\",\n",
    "    \"What are Bloom Filters, and how are they used in applications like spell checkers and caching?\",\n",
    "    \"How does a blockchain achieve consensus, and what are the differences between Proof of Work and Proof of Stake?\",\n",
    "    \"What is the difference between symmetric and asymmetric encryption, and when would you use each?\",\n",
    "    \"What are software design patterns, and why is the Singleton pattern considered controversial?\",\n",
    "    \"How does garbage collection work in programming languages like Java, and what are the trade-offs involved?\",\n",
    "    \"What is a finite state machine, and how are they used in designing software systems?\",\n",
    "    \"How does Agile methodology differ from Waterfall in software development workflows?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b35e789c-2a5f-462c-b661-81bc9c4c231d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with text_paragraphs...\n",
      "Number of Clusters Formed: 4\n",
      "\n",
      "Cluster 0 (2 items):\n",
      "  - What are network sockets, and how do they facilitate communication between distributed systems?\n",
      "  - What are Content Delivery Networks (CDNs), and how do they handle geographically distributed traffic?\n",
      "\n",
      "Cluster 2 (4 items):\n",
      "  - What is a web server, and how does it differ from an application server in terms of functionality?\n",
      "  - What is the difference between TCP and UDP, and how do they affect data transmission reliability?\n",
      "  - Explain the role of SSL/TLS in securing HTTP connections.\n",
      "  - What are RESTful APIs, and how do they differ from GraphQL in terms of data fetching flexibility?\n",
      "\n",
      "Cluster 1 (3 items):\n",
      "  - How does DNS caching improve website load times, and what are its potential downsides?\n",
      "  - How does NAT (Network Address Translation) work, and what are its implications for IPv4 addresses?\n",
      "  - What are the advantages of using WebSocket over HTTP for real-time applications?\n",
      "\n",
      "Cluster 3 (1 items):\n",
      "  - How does a firewall filter incoming and outgoing traffic, and what are common configurations for web security?\n"
     ]
    }
   ],
   "source": [
    "# Define a testing function\n",
    "def test_clustering(paragraphs, max_k=10):\n",
    "    \"\"\"\n",
    "    Tests the clustering implementation with the given paragraphs.\n",
    "    \n",
    "    Parameters:\n",
    "    - paragraphs (list): List of text paragraphs to cluster.\n",
    "    - max_k (int): Maximum number of clusters for the elbow method.\n",
    "    \"\"\"\n",
    "    clustering = DoubtClustering(max_k=max_k)\n",
    "    clusters = clustering.cluster(paragraphs)\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"Number of Clusters Formed:\", len(clusters))\n",
    "    for cluster_id, cluster_paragraphs in clusters.items():\n",
    "        print(f\"\\nCluster {cluster_id} ({len(cluster_paragraphs)} items):\")\n",
    "        for paragraph in cluster_paragraphs:\n",
    "            print(f\"  - {paragraph}\")\n",
    "\n",
    "print(\"\\nTesting with text_paragraphs...\")\n",
    "test_clustering(test_paragraphs_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "daaad164-a4e1-4cf7-a9fc-17b6ed0d6bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rijusmit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Option 1: TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust features for scalability\n",
    "X = vectorizer.fit_transform(text_paragraphs).toarray()\n",
    "\n",
    "# Option 2: Sentence Embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight transformer for embeddings\n",
    "X = model.encode(text_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6774a1c-f096-4cb9-9aed-a53e7d96fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d46a81a-f89d-4e13-b8e9-86f3061008e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
